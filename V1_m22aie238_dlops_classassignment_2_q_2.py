# -*- coding: utf-8 -*-
"""M22AIE238_DLOps_ClassAssignment_2_Q_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E9f6ktOgXbn3rwhZD4VAArSkeW1CsjlV
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader, Subset
from torchvision.models import resnet50
from torchvision import datasets, transforms, models
import matplotlib.pyplot as plt

# Define a simpler transformation for MNIST
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)) # MNIST mean and std
])

# Load MNIST dataset
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Create a subset for faster training
train_subset = Subset(train_dataset, range(2000))  # Using first 10,000 samples for training
test_subset = Subset(test_dataset, range(500))  # Using first 5,000 samples for testing

# Define data loaders
train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)

# Use a smaller model for MNIST
model = models.resnet18(pretrained=False, num_classes=10) # ResNet18 is smaller and more efficient for MNIST
# Modify the first layer to accept 1-channel input
model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)


import torch.optim as optim

# Define optimizers
optimizers = {
    'Adam': optim.Adam(model.parameters(), lr=0.001),
    'Adagrad': optim.Adagrad(model.parameters(), lr=0.001),
    'RMSprop': optim.RMSprop(model.parameters(), lr=0.001)
}



# Training loop for each optimizer
num_epochs = 3
results = {}

for optimizer_name, optimizer in optimizers.items():
    print(f"Training with {optimizer_name} optimizer...")

    train_losses = []
    train_accs = []

    for epoch in range(num_epochs):
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)

        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')

    results[optimizer_name] = {'train_losses': train_losses, 'train_accs': train_accs}

# Plotting training loss and accuracy curves for each optimizer
plt.figure(figsize=(12, 5))
for optimizer_name, result in results.items():
    plt.subplot(1, 2, 1)
    plt.plot(result['train_losses'], label=f'{optimizer_name} Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Curve')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(result['train_accs'], label=f'{optimizer_name} Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training Accuracy Curve')
    plt.legend()

plt.show()

# Calculate top-3 test accuracy for each optimizer
top5_test_accs = {}
for optimizer_name, optimizer in optimizers.items():
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.topk(outputs, 5, dim=1)
            total += labels.size(0)
            for i in range(labels.size(0)):
                if labels[i] in predicted[i]:
                    correct += 1
    top5_acc = correct / total
    top5_test_accs[optimizer_name] = top5_acc
    print(f'Top-3 Test Accuracy using {optimizer_name} optimizer: {top5_acc:.4f}')











